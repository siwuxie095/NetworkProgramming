package com.siwuxie095.network.chapter15th.example8th;

/**
 * @author Jiajing Li
 * @date 2020-12-05 20:44:13
 */
public class Main {

    /**
     * 结果
     *
     * 下面将量化一些使用 Netty 的过程中所观察到的一些成果。
     *
     *
     * 1、性能比较
     *
     * 一种测量 Thrift 服务器性能的方式是对于空操作的基准测试。这种基准测试使用了长时间运行的客户端，这些客户端
     * 不间断地对发送回空响应的服务器进行 Thrift 调用。虽然这种测量方式对于大部分的实际 Thrift 服务来说，不是
     * 真实意义上的性能测试，但是它仍然很好地度量了 Thrift 服务的最大潜能，而且提高这一基准，通常也就意味着减少
     * 了该框架本身的 CPU 使用。
     *
     * 在这个基准测试下，Nifty 的性能优于所有其他基于 NIO 的 Thrift 服务器（TNonblockingServer、
     * TThreadedSelectorServer 以及 TThreadPoolServer）的实现。它甚至轻松地击败了以前的 Java 服务器实现
     * （他们内部使用的一个 Nifty 之前的服务器实现，基于原始的NIO 以及直接缓冲区）。
     *
     * 他们所测试过的唯一能够和 Nifty 相提并论的 Java 服务器是 TThreadPoolServer。这个服务器实现使用了原始
     * 的 OIO，并且在一个专门的线程上运行每个连接。这使得它在处理少量的连 接时表现不错，然而，使用 OIO，当你的
     * 服务器需要处理大量的并发连接时，你将很容易遇到伸缩性问题。
     *
     * Nifty 甚至击败了之前的 C++服务器实现，这是他们开始开发 Nifty 时最夺目的一点，虽然它相对于他们的下一代
     * C++ 服务器框架还有一些差距，但至少也大致相当。
     *
     *
     * 2、稳定性问题的例子
     *
     * 在 Nifty 之前，他们在 Facebook 的许多主要的 Java 服务都使用了一个较老的、自定义的基于 NIO 的 Thrift
     * 服务器实现，它的工作方式类似于 Nifty。该实现是一个较旧的代码库，有更多的时间成熟，但是由于它的异步 I/O
     * 处理代码是从零开始构建的，而且因为 Nifty 是构建在 Netty 的异步 I/O 框架的坚实基础之上的，所以（相比之
     * 下）它的问题也就少了很多。
     *
     * 他们的一个自定义的消息队列服务是基于那个较旧的框架构建的，而它开始遭受一种套接字泄露。大量的连接都停留在
     * 了 CLOSE_WAIT 状态，这意味着服务器接收了客户端已经关闭了套接字的通知，但是服务器从来不通过其自身的调用
     * 来关闭套接字进行回应。这使得这些套接字都停滞在了 CLOSE_WAIT 状态。
     *
     * 问题发生得很慢。在处理这个服务的整个机器集群中，每秒可能有数以百万计的请求，但是通常在一个服务器上只有一
     * 个套接字会在一个小时之内进入这个状态。这不是一个迫在眉睫的问题，因为在那种速率下，在一个服务器需要重启前，
     * 将需要花费很长的时间，但是这也复杂化了追查原因的过程。彻底地挖掘代码也没有带来太大的帮助：最初的几个地方
     * 看起来可疑，但是最终都被排除了，而他们也并没有定位到问题所在。
     *
     * 最终，他们将该服务迁移到了 Nifty 之上。转换（包括在预发环境中进行测试）花了不到一天的时间，而这个问题就
     * 此消失了。使用 Nifty，他们就真的再也没见过类似的问题了。
     *
     * 这只是在直接使用 NIO 时可能会出现的微妙 bug 的一个例子，而且它类似于那些在他们的 C++ Thrift 框架稳定
     * 的过程中，不得不一次又一次地解决的 bug。但是他认为这是一个很好的例子，它说明了通过使用 Netty 是如何帮
     * 助他们利用它多年来收到的稳定性修复的。
     *
     *
     * 3、改进 C++ 实现的超时处理
     *
     * Netty 还通过为改进他们的 C++ 框架提供一些启发间接地帮助了他们。一个这样的例子是基于散列轮的计时器。他们
     * 的 C++ 框架使用了来自于 libevent 的超时事件来驱动客户端以及服务器的超时，但是为每个请求都添加一个单独的
     * 超时被证明是十分昂贵的，因此他们一直都在使用他们称之为超时集的东西。其思想是：一个到特定服务的客户端连接，
     * 对于由该客户端发出的每个请求，通常都具有相同的接收超时，因此对于一组共享了相同的时间间隔的超时集合，他们
     * 仅维护一个真正的计时器事件。每个新的超时都将被保证会在对于该超时集合的现存的超时被调度之后触发，因此当每
     * 个超时过期或者被取消时，他们将只会安排下一个超时。
     *
     * 然而，他们的用户偶尔想要为每个调用都提供单独的超时，为在相同连接上的不同的请求设置不同的超时值。在这种情
     * 况下，使用超时集合的好处就消失了，因此他们尝试了使用单独的计时器事件。在大量的超时被同时调度时，他们开始
     * 看到了性能问题。他们知道 Nifty 不会碰到这个问题，除了它不使用超时集的这个事实 —— Netty 通过它的
     * HashedWheelTimer 解决了该问题。因此，带着来自 Netty 的灵感，他们为他们的 C++ Thrift 框架添加了一个
     * 基于散列轮的计时器，并解决了可变的每请求（per-request）超时时间间隔所带来的性能问题。
     *
     * PS：有关 HashedWheelTimer 类的更多的信息，参见 https://netty.io/4.1/api/io/netty/util/HashedWheelTimer
     *
     *
     * 4、未来基于 Netty 4 的改进
     *
     * Nifty 目前运行在 Netty 3 上，这对他们来说已经很好了，但是他们已经有一个基于 Netty 4 的移植版本准备好
     * 了，现在第 4 版的 Netty 已经稳定下来了，他们很快就会迁移过去。他们热切地期待着 Netty 4 的 API 将会带
     * 给他们的一些益处。
     *
     * 一个他们计划如何更好地利用 Netty 4 的例子是实现更好地控制哪个线程将管理一个给定的连接。他们希望使用这项
     * 特性，可以使服务器的处理器方法能够从和该服务器调用所运行的 I/O 线程相同的线程开始异步的客户端调用。这是
     * 那些专门的 C++ 服务器（如 Thrift 请求路由器）已经能够利用的特性。
     * 
     * 从该例子延伸开来，他们也期待着能够构建更好的客户端连接池，使得能够把现有的池化连接迁移到期望的 I/O 工作
     * 线程上，这在第 3 版的 Netty 中是不可能做到的。
     */
    public static void main(String[] args) {

    }

}
